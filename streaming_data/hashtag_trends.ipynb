{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://asus:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x250180f78b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate();\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=SparkContext(appName=\"Twitter Data Analysis\")\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "ssc=StreamingContext(sc,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "socket_stream = ssc.socketTextStream(\"127.0.0.1\",5555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines=socket_stream.window(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = lines.flatMap(lambda text:text.split(\" \")).filter(lambda word:word.lower().startswith('#')).map(lambda word:(word.lower(),1)).reduceByKey(lambda a,b:a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_counts_sorted_dstream=hashtags.transform(lambda foo:foo.sortBy(lambda x:x[0].lower()).sortBy(lambda x:x[1],ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_counts_sorted_dstream.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2021-08-15 12:46:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-08-15 12:46:10\n",
      "-------------------------------------------\n",
      "('#netzfund', 1)\n",
      "('#traitordonthelyingthievingcon', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-08-15 12:46:20\n",
      "-------------------------------------------\n",
      "('#afganistan:le', 1)\n",
      "('#biden', 1)\n",
      "('#covid19,', 1)\n",
      "('#instablockrt', 1)\n",
      "('#lafc', 1)\n",
      "('#netzfund', 1)\n",
      "('#oannglückwunsch', 1)\n",
      "('#traitordonthelyingthievingcon', 1)\n",
      "('#trump', 1)\n",
      "('#trumpreinstatement', 1)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-08-15 12:46:30\n",
      "-------------------------------------------\n",
      "('#afganistan:le', 1)\n",
      "('#biden', 1)\n",
      "('#covid19,', 1)\n",
      "('#instablockrt', 1)\n",
      "('#lafc', 1)\n",
      "('#netzfund', 1)\n",
      "('#oannglückwunsch', 1)\n",
      "('#reinstatementday', 1)\n",
      "('#traitordonthelyingthievingcon', 1)\n",
      "('#trump', 1)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-08-15 12:46:40\n",
      "-------------------------------------------\n",
      "('#bidenmustresign', 2)\n",
      "('#trumpworld', 2)\n",
      "('#afganistan:le', 1)\n",
      "('#biden', 1)\n",
      "('#covid19,', 1)\n",
      "('#ferragosto2021', 1)\n",
      "('#gopcorruptionovercountry', 1)\n",
      "('#goppartyoverdemocracy', 1)\n",
      "('#instablockrt', 1)\n",
      "('#lafc', 1)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-08-15 12:46:50\n",
      "-------------------------------------------\n",
      "('#trump', 7)\n",
      "('#itiswhatitis…', 3)\n",
      "('#mylettertorepublicans', 3)\n",
      "('#trumpisguilty', 3)\n",
      "('#bidenmustresign', 2)\n",
      "('#covid19,', 2)\n",
      "('#trumpworld', 2)\n",
      "('#afganistan:le', 1)\n",
      "('#biden', 1)\n",
      "('#ferragosto2021', 1)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-08-15 12:47:00\n",
      "-------------------------------------------\n",
      "('#trump', 11)\n",
      "('#bidenmustresign', 5)\n",
      "('#itiswhatitis…', 5)\n",
      "('#mylettertorepublicans', 5)\n",
      "('#trumpisguilty', 5)\n",
      "('#taliban', 3)\n",
      "('#covid19,', 2)\n",
      "('#fullforensicaudits', 2)\n",
      "('#teaparty', 2)\n",
      "('#trumpisalaughingstock', 2)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-08-15 12:47:10\n",
      "-------------------------------------------\n",
      "('#trump', 14)\n",
      "('#bidenmustresign', 13)\n",
      "('#taliban', 9)\n",
      "('#itiswhatitis…', 5)\n",
      "('#mylettertorepublicans', 5)\n",
      "('#trumpisguilty', 5)\n",
      "('#afghanistan', 3)\n",
      "('#trumpworld', 3)\n",
      "('#covid19,', 2)\n",
      "('#fullforensicaudits', 2)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-08-15 12:47:20\n",
      "-------------------------------------------\n",
      "('#trump', 21)\n",
      "('#bidenmustresign', 17)\n",
      "('#taliban', 13)\n",
      "('#afghanistan', 7)\n",
      "('#itiswhatitis…', 5)\n",
      "('#mylettertorepublicans', 5)\n",
      "('#trumpisguilty', 5)\n",
      "('#maga', 3)\n",
      "('#trumpworld', 3)\n",
      "('#bidenmustresign,', 2)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-08-15 12:47:30\n",
      "-------------------------------------------\n",
      "('#trump', 36)\n",
      "('#bidenmustresign', 21)\n",
      "('#taliban', 17)\n",
      "('#afghanistan', 10)\n",
      "('#bolsonaro', 6)\n",
      "('#covid19,', 5)\n",
      "('#itiswhatitis…', 5)\n",
      "('#mylettertorepublicans', 5)\n",
      "('#trumpisguilty', 5)\n",
      "('#biden', 4)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-08-15 12:47:40\n",
      "-------------------------------------------\n",
      "('#trump', 56)\n",
      "('#bidenmustresign', 37)\n",
      "('#taliban', 22)\n",
      "('#afghanistan', 17)\n",
      "('#covid19,', 11)\n",
      "('#biden', 8)\n",
      "('#bolsonaro', 8)\n",
      "('#biden...', 6)\n",
      "('#maga', 6)\n",
      "('#itiswhatitis…', 5)\n",
      "...\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o62.awaitTermination.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"C:\\opt\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\streaming\\util.py\", line 68, in call\n    r = self.func(t, *rdds)\n  File \"C:\\opt\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\streaming\\dstream.py\", line 666, in <lambda>\n    self.func = lambda t, rdd: func(t, prev_func(t, rdd))\n  File \"C:\\opt\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\streaming\\dstream.py\", line 297, in <lambda>\n    func = lambda t, rdd: oldfunc(rdd)\n  File \"<ipython-input-8-90847648889e>\", line 1, in <lambda>\n    author_counts_sorted_dstream=hashtags.transform(lambda foo:foo.sortBy(lambda x:x[0].lower()).sortBy(lambda x:x[1],ascending=False))\n  File \"C:\\opt\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 812, in sortBy\n    return self.keyBy(keyfunc).sortByKey(ascending, numPartitions).values()\n  File \"C:\\opt\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 778, in sortByKey\n    rddSize = self.count()\n  File \"C:\\opt\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 1235, in count\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"C:\\opt\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 1224, in sum\n    return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\n  File \"C:\\opt\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 1078, in fold\n    vals = self.mapPartitions(func).collect()\n  File \"C:\\opt\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 949, in collect\n    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n  File \"C:\\opt\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\", line 1304, in __call__\n    return_value = get_return_value(\n  File \"C:\\opt\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\", line 111, in deco\n    return f(*a, **kw)\n  File \"C:\\opt\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 93 in stage 169.0 failed 1 times, most recent failure: Lost task 93.0 in stage 169.0 (TID 2164) (asus executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:115)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 19 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.GeneratedMethodAccessor53.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:115)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 19 more\r\n\n\r\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\r\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\r\n\tat org.apache.spark.streaming.api.python.PythonTransformedDStream.compute(PythonDStream.scala:246)\r\n\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\r\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\r\n\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\r\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\r\n\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\r\n\tat scala.Option.orElse(Option.scala:447)\r\n\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)\r\n\tat org.apache.spark.streaming.DStreamGraph.$anonfun$generateJobs$2(DStreamGraph.scala:123)\r\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)\r\n\tat scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:75)\r\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)\r\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)\r\n\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\r\n\tat org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:122)\r\n\tat org.apache.spark.streaming.scheduler.JobGenerator.$anonfun$generateJobs$1(JobGenerator.scala:252)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:250)\r\n\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:186)\r\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:91)\r\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:90)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-6cfd70f7bded>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mssc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\streaming\\context.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m    198\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o62.awaitTermination.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"C:\\opt\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\streaming\\util.py\", line 68, in call\n    r = self.func(t, *rdds)\n  File \"C:\\opt\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\streaming\\dstream.py\", line 666, in <lambda>\n    self.func = lambda t, rdd: func(t, prev_func(t, rdd))\n  File \"C:\\opt\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\streaming\\dstream.py\", line 297, in <lambda>\n    func = lambda t, rdd: oldfunc(rdd)\n  File \"<ipython-input-8-90847648889e>\", line 1, in <lambda>\n    author_counts_sorted_dstream=hashtags.transform(lambda foo:foo.sortBy(lambda x:x[0].lower()).sortBy(lambda x:x[1],ascending=False))\n  File \"C:\\opt\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 812, in sortBy\n    return self.keyBy(keyfunc).sortByKey(ascending, numPartitions).values()\n  File \"C:\\opt\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 778, in sortByKey\n    rddSize = self.count()\n  File \"C:\\opt\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 1235, in count\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"C:\\opt\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 1224, in sum\n    return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\n  File \"C:\\opt\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 1078, in fold\n    vals = self.mapPartitions(func).collect()\n  File \"C:\\opt\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 949, in collect\n    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n  File \"C:\\opt\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\", line 1304, in __call__\n    return_value = get_return_value(\n  File \"C:\\opt\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\", line 111, in deco\n    return f(*a, **kw)\n  File \"C:\\opt\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 93 in stage 169.0 failed 1 times, most recent failure: Lost task 93.0 in stage 169.0 (TID 2164) (asus executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:115)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 19 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.GeneratedMethodAccessor53.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:115)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 19 more\r\n\n\r\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\r\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\r\n\tat org.apache.spark.streaming.api.python.PythonTransformedDStream.compute(PythonDStream.scala:246)\r\n\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\r\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\r\n\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\r\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\r\n\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\r\n\tat scala.Option.orElse(Option.scala:447)\r\n\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)\r\n\tat org.apache.spark.streaming.DStreamGraph.$anonfun$generateJobs$2(DStreamGraph.scala:123)\r\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)\r\n\tat scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:75)\r\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)\r\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)\r\n\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\r\n\tat org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:122)\r\n\tat org.apache.spark.streaming.scheduler.JobGenerator.$anonfun$generateJobs$1(JobGenerator.scala:252)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:250)\r\n\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:186)\r\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:91)\r\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:90)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2021-08-15 12:48:00\n",
      "-------------------------------------------\n",
      "('#trump', 62)\n",
      "('#bidenmustresign', 44)\n",
      "('#taliban', 32)\n",
      "('#afghanistan', 20)\n",
      "('#biden', 17)\n",
      "('#covid19,', 13)\n",
      "('#biden...', 7)\n",
      "('#bolsonaro', 7)\n",
      "('#deathsantis', 7)\n",
      "('#kremlin', 7)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-08-15 12:48:10\n",
      "-------------------------------------------\n",
      "('#trump', 75)\n",
      "('#bidenmustresign', 55)\n",
      "('#taliban', 35)\n",
      "('#afghanistan', 24)\n",
      "('#biden', 21)\n",
      "('#covid19,', 17)\n",
      "('#サイモン・パークス情報', 11)\n",
      "('#kremlin', 10)\n",
      "('#bolsonaro', 9)\n",
      "('#biden...', 8)\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
